---
title: "MAT4376 - Project"
author: "Jaye Foster"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(glmnet)
library(ggplot2)
library(dplyr)
```

Elastic net is a penalized regression method that combines the penalties
of RIDGE and LASSO to achieve sparcity and deal with linearly dependent
predictors. We will analyze the performance of Elastic Net to better
understand how the selection of $\alpha$, and correlation between
predictors can effect the MSE and number of predictors deemed
significant.

We begin with some theoretical background on LASSO and RIDGE to create
the Elastic Net method. We consider the linear model
$$\boldsymbol{Y}=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}$$
where 

* $\boldsymbol{Y}$ is the $n \times 1$ response vector;
* $\boldsymbol{X}$ is the $n \times p$ design matrix;
* $\boldsymbol{\beta}$ is the $p \times 1$ vector of regression coefficients;
* $\boldsymbol{\epsilon}$ is the $n \times 1$ error vector.


In the case when $n>p$, we can use the classical method Ordinary Least Squares (OLS) to estimate the regression coefficients
$\boldsymbol{\beta}$. The OLS estimator of $\boldsymbol{\beta}$ is given
by minimizing the quadratic loss function 
$$\hat{\boldsymbol{\beta}}_{OLS} = \text{argmin}_{\boldsymbol{\beta}} \{ L(\boldsymbol{\beta}) \}\\
= \text{argmin}_{\boldsymbol{\beta}} \left\{ \frac{1}{2n} \left\| \boldsymbol{Y} - \boldsymbol{X}\boldsymbol{\beta}  \right\|^2 _2 \right\}\\$$
The closed-form solution can be given by
$$\hat{\boldsymbol{\beta}}_{OLS} = (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{X}^T \boldsymbol{Y}$$

If the data is high-dimensional, i.e. $n<p$, we cannot use OLS. Instead, we must assume that only some of the $\beta_j$ are non-zero, an assumption  known as sparcity. To achieve this, we impose a constraint on the norm of $\boldsymbol{\beta}$. Instead of minimizing just the quadratic loss function $L(\beta)$, the new estimator is found by minimizing 
$$L(\boldsymbol{\beta})+ \lambda \| \boldsymbol{\beta} \|$$ where $\lambda$ is the regularization parameter that controls how many of the $\beta_j$ are non-zero. 

We will consider two possible norms for our penalty, $L_1$ and $L_2$, which give the LASSO and RIDGE estimators. 


$$ \hat{\boldsymbol{\beta}}_{LASSO}= \text{argmin}_{\boldsymbol{\beta}} \left\{ \frac{1}{2n} \left\| \boldsymbol{Y} - \boldsymbol{X}\boldsymbol{\beta}  \right\|^2 _2  + \lambda \| \boldsymbol{\beta} \|_1 \right\}$$


$$ \hat{\boldsymbol{\beta}}_{RIDGE}= \text{argmin}_{\boldsymbol{\beta}} \left\{ \frac{1}{2n} \left\| \boldsymbol{Y} - \boldsymbol{X}\boldsymbol{\beta}  \right\|^2 _2  + \lambda \| \boldsymbol{\beta} \|_2 \right\}$$

We compare these methods to see that LASSO can achieve sparcity while RIDGE cannot. That is, LASSO can set some of the coefficients $\beta_j$ to exactly zero while RIDGE only shrinks them smaller and smaller. When some of the predictors are strongly correlated the LASSO solution is not unique while the RIDGE solution is. As a result, neither LASSO nor RIDGE can deal with linearly dependent predictors while also achieving sparcity.

To solve this problem, we create a new estimator that combines the LASSO and RIDGE penalties. This is the idea behind Elastic Net. Given a mixture parameter $\alpha \in [0,1]$ and a regularization parameter $\lambda >0$, the Elastic Net estimator is given by;
$$\hat{\boldsymbol{\beta}}_{EN}=\text{argmin}_{\boldsymbol{\beta}} \left\{ \frac{1}{2n} \left\| \boldsymbol{Y} - \boldsymbol{X}\boldsymbol{\beta}  \right\|^2 _2 + \lambda [ \alpha \| \boldsymbol{\beta} \|_1 + (1- \alpha) \| \boldsymbol{\beta} \|_2 ] \right\}$$
Now we have an estimator that can achieve sparcity and handle linearly dependent predictors at the same time. Let's see how it works in practice.

Suppose we have a $n=100$ observations from the following model where $X_1$ and $X_2$ are highly correlated and $X_3$ is uncorrelated. 

$$Y=\beta_0 + \beta_1 X_{1} + \beta_2 X_{2} + \beta_3 X_{3},$$ $$\beta_0=1,\beta_1=2,\beta_2=1,\beta_3=3$$
For $\alpha_1=0.1, \alpha_2=0.5, \alpha_3=0.9$, we can select $\lambda$ by cross-validation and find the elastic net estimator for each. 



```{r }
n<-100 #number of observations
beta<-c(1,2,1,3) #true values of beta
rho<-0.90 #values of cov(X_1, X_2)
alpha<-c(0.1, 0.3, 0.9) #values of alpha


elastic_net3<-function(n, beta, rho, alpha, K){
  num_alpha<-length(alpha)
  num_rho<-length(rho)
  Rho<-rep(0, num_alpha)                                                          #build table to hold squared errors
  Alpha<-rep(0,num_rho)
  NumPredictors<-rep(0, num_rho*num_alpha)
  MSE_K<-cbind(Rho, Alpha, NumPredictors, matrix(0, num_rho*num_alpha, K))
  
  for(k in 1:K){
    row=1
    for(j in 1:num_rho){
      cov<-matrix(data = c(1, rho[j], 0, rho[j], 1, 0, 0, 0, 1), nrow=3, ncol=3)  #build covariance matrix
      X<-mvrnorm(n, mu=c(0,0,0), Sigma =cov)                                      #simulate X
      design<-cbind(rep(1,n), X )
      Y<-design%*%beta+rnorm(n)                                                   #simulate Y
      for(i in 1:num_alpha){
        fit_i<-glmnet(X, Y, family="gaussian", alpha=alpha[i])                 #fit model with cv
        plot(fit_i, xvar="lambda", label=TRUE)
        fit_i_coef<-coef(fit_i, s="lambda.min")                                   #extract coefficients
        yhat<-design%*%fit_i_coef                                                 #extract fitted values
        if(k<=1){
          MSE_K[row,1]<-rho[j]
          MSE_K[row,2]<-alpha[i]                                                  #fill MSE table with alpha and rho
          MSE_K[row,3]<-length(fit_i_coef@x)}
        MSE_K[row,(k+3)]<-mean(((Y-yhat)^2))                                      #fill MSE table with squared errors
        row=row+1
      }}}
  results<-cbind(MSE_K[,(1:3)], 0)                                                #build results table
  if(K==1){
    results[,4]<-MSE_K[,4]
  }
  else{
  results[,4]<-rowMeans(MSE_K[,(4:(K+3))])                # fill results table with MSE
  }
  colnames(results)[4]<-"MSE"
  
  return(as.data.frame(results))
}

EN_p_3_one_iter<-elastic_net3(n, beta, rho, alpha,1)
ggplot(data=EN_p_3_one_iter, aes(x=Alpha, y=MSE))+geom_line(alpha=0.4)



```
Now, lets see what happens in the high-dimensional case. 

Suppose we have a dataset with $p=1000$ covariates, some of which are correlated, and $n=100$ observations. We perform Elastic net with $\alpha_1=0.1, \alpha_2=0.5, \alpha_3=0.9$ and $\lambda$ selected by cross-validation.  
```{r }
n<-100
p<-1000
beta<-c(runif(6,1,2), rep(0,95), runif(5, 1,2), rep(0,600), runif(5, 1,2), rep(0,290))
rho<-seq(0, 0.9, 0.1)
alpha<-seq(0, 1, 0.1)




elastic_net_p<-function(n, p, beta, rho, alpha, K){
  num_alpha<-length(alpha)
  num_rho<-length(rho)
  Rho<-rep(0, num_alpha)
  Alpha<-rep(0,num_rho)
  MSE<-rep(0, num_rho*num_alpha)
  NumPredictors<-rep(0, num_rho*num_alpha)
  
  MSE_K<-cbind(Rho, Alpha, NumPredictors, matrix(0, num_rho*num_alpha, K))
  
    for(k in 1:K){
    row=1
    for(j in 1:num_rho){
      cov<-matrix(rho[j], p, p)
      for(l in 1:p){
        cov[l,l]<-1}
      X<-mvrnorm(n, mu=c(0,0,0), Sigma =cov)
      design<-cbind(rep(1,n), X )
      Y<-design%*%beta+rnorm(n)
      for(i in 1:num_alpha){
        fit_i<-glmnet(X, Y, family="gaussian", alpha=alpha[i])
        fit_i_coef<-coef(fit_i, s="lambda.min")
        yhat<-design%*%fit_i_coef
        if(k<=1){
          MSE_K[row,1]<-rho[j]
          MSE_K[row,2]<-alpha[i]
          MSE_K[row,3]<-length(fit_i_coef@x)}
        MSE_K[row,(k+3)]<-mean(((Y-yhat)^2))
        row=row+1
      }}}
  results<-cbind(MSE_K[,(1:3)], 0)
  results[,4]<-rowMeans(MSE_K[,(4:K)])
  colnames(results)[4]<-"MSE"
  return(as.data.frame(results))
}

lambda<-matrix(0,length(alpha),length(rho))
mse<-matrix(0,length(alpha),length(rho))
q<-matrix(0,length(alpha), length(rho))



for(j in 1:length(rho)){
  cov=matrix(c(1, rho[j], rho[j], rho[j], rho[j], 
              rho[j], 1, rho[j], rho[j], rho[j],
              rho[j], rho[j], 1, rho[j], rho[j],
              rho[j], rho[j], rho[j], 1, rho[j],
              rho[j], rho[j], rho[j], rho[j], 1), nrow=5, ncol=5)
  X1_5<-mvrnorm(n, mu=c(0,0,0,0,0), Sigma =cov )
  X6_1000<-mvrnorm(n, mu=rep(0, 995), Sigma = diag(rep(1, 995)))
  X<-cbind(X1_5, X6_1000)
  design<-cbind(rep(1,n), X)
  Y<-design%*%beta+rnorm(n)

  for(i in 1:length(alpha)){
    fit_i<-cv.glmnet(X, Y, family="gaussian", alpha=alpha[i])
    lambda[i,j]<-fit_i$lambda.min
    fit_i_coef<-coef(fit_i, s=lambda[i])
    yhat<-design%*%fit_i_coef
    mse[i,j]<-mean((Y-yhat)^2)
  }}



```

```{r }
n<-100
beta<-c(5,1,3,2)
rho<- seq(-0.9, 0.9, 0.25)
alpha<-seq(0.1, 0.9, 0.1)

elastic_net3<-function(n, beta, rho, alpha, K){
  num_alpha<-length(alpha)
  num_rho<-length(rho)
  Rho<-rep(0, num_alpha)
  Alpha<-rep(0,num_rho)
  MSE<-rep(0, num_rho*num_alpha)
  NumPredictors<-rep(0, num_rho*num_alpha)
  
  MSE_K<-cbind(Rho, Alpha, NumPredictors, matrix(0, num_rho*num_alpha, K))
  
  for(k in 1:K){
    row=1
    for(j in 1:num_rho){
      cov<-matrix(data = c(1, rho[j], 0, rho[j], 1, 0, 0, 0, 1), nrow=3, ncol=3)
      X<-mvrnorm(n, mu=c(0,0,0), Sigma =cov)
      design<-cbind(rep(1,n), X )
      Y<-design%*%beta+rnorm(n)
      for(i in 1:num_alpha){
        fit_i<-cv.glmnet(X, Y, family="gaussian", alpha=alpha[i])
        fit_i_coef<-coef(fit_i, s="lambda.min")
        yhat<-design%*%fit_i_coef
        if(k<=1){
          MSE_K[row,1]<-rho[j]
          MSE_K[row,2]<-alpha[i]
          MSE_K[row,3]<-length(fit_i_coef@x)}
        MSE_K[row,(k+3)]<-mean(((Y-yhat)^2))
        row=row+1
      }}}
  results<-cbind(MSE_K[,(1:3)], 0)
  results[,4]<-rowMeans(MSE_K[,(4:K+3)])
  colnames(results)[4]<-"MSE"
  
  return(as.data.frame(results))
}
#hundered_iter<-elastic_net3(n, beta, rho, alpha, 100)
#fifty_iter<-elastic_net3(n, beta, rho, alpha, 50)
ten_iter<-elastic_net3(n, beta, rho, alpha, 10)

ggplot(data=hundered_iter, aes(x=Rho, y=MSE, group=factor(Alpha)))+geom_line(aes(colour=factor(Alpha)),  alpha=0.4)
ggplot(data=fifty_iter, aes(x=Rho, y=MSE, group=factor(Alpha)))+geom_line(aes(colour=factor(Alpha)),  alpha=0.4)
ggplot(data=ten_iter, aes(x=Rho, y=MSE, group=factor(Alpha)))+geom_line(aes(colour=factor(Alpha)),  alpha=0.4)




```