---
title: "MAT4376 - Project"
author: "Jaye Foster"
date: "`r Sys.Date()`"
header-includes:
  - \usepackage{amsmath}
output:
  pdf_document: default
  html_document: default
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(glmnet)
library(ggplot2)
library(dplyr)
```
#Introduction

Elastic net is a regression method that provides a possible solution to two common problems in high-dimensional data analysis; sparsity and linear dependence in covariates. By combining the penalties of LASSO and RIDGE regression, Elastic Net can take advantage of the strengths of each method to varying extents. An additional parameter $\alpha$ controls the weight given to the LASSO and RIDGE portions of the penalty, and must be chosen carefully. We will see how Elastic Net performs with different levels of correlation between predictors and choices of $\alpha$. 

#Background Theory

We begin with some theoretical background on LASSO and RIDGE to see how the Elastic Net method is derived. Consider the linear model
$$\boldsymbol{Y}=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}$$
where 

* $\boldsymbol{Y}$ is the $n \times 1$ response vector;
* $\boldsymbol{X}$ is the $n \times p$ design matrix;
* $\boldsymbol{\beta}$ is the $p \times 1$ vector of regression coefficients;
* $\boldsymbol{\epsilon}$ is the $n \times 1$ error vector.


In the case when $n>p$, we can use the classical method Ordinary Least Squares (OLS) to estimate the regression coefficients
$\boldsymbol{\beta}$. The OLS estimator of $\boldsymbol{\beta}$ is given
by minimizing the quadratic loss function 
$$\hat{\boldsymbol{\beta}}_{OLS} = \text{argmin}_{\boldsymbol{\beta}} \{ L(\boldsymbol{\beta}) \}\\
= \text{argmin}_{\boldsymbol{\beta}} \left\{ \frac{1}{2n} \left\| \boldsymbol{Y} - \boldsymbol{X}\boldsymbol{\beta}  \right\|^2 _2 \right\}\\$$
The closed-form solution can be given by
$$\hat{\boldsymbol{\beta}}_{OLS} = (\boldsymbol{X}^T \boldsymbol{X})^{-1} \boldsymbol{X}^T \boldsymbol{Y}$$

If the data is high-dimensional, i.e. $n<p$, we cannot use OLS. Instead, we must assume that only some of the $\beta_j$ are non-zero, an assumption known as sparsity. To achieve this, we impose a constraint on the norm of $\boldsymbol{\beta}$. Instead of minimizing just the quadratic loss function $L(\beta)$, the new estimator is found by minimizing 
$$L(\boldsymbol{\beta})+ \lambda \| \boldsymbol{\beta} \|$$ where $\lambda$ is the regularization parameter that controls how many of the $\beta_j$ are non-zero. 

We will consider two possible norms for our penalty, $L_1$ and $L_2$, which give the LASSO and RIDGE estimators. 


$$ \hat{\boldsymbol{\beta}}_{LASSO}= \text{argmin}_{\boldsymbol{\beta}} \left\{ \frac{1}{2n} \left\| \boldsymbol{Y} - \boldsymbol{X}\boldsymbol{\beta}  \right\|^2 _2  + \lambda \| \boldsymbol{\beta} \|_1 \right\}$$


$$ \hat{\boldsymbol{\beta}}_{RIDGE}= \text{argmin}_{\boldsymbol{\beta}} \left\{ \frac{1}{2n} \left\| \boldsymbol{Y} - \boldsymbol{X}\boldsymbol{\beta}  \right\|^2 _2  + \lambda \| \boldsymbol{\beta} \|_2 \right\}$$

We compare these methods to see that LASSO can achieve sparsity while RIDGE cannot. That is, LASSO can set some of the coefficients $\beta_j$ to exactly zero while RIDGE only shrinks them smaller and smaller. When some of the predictors are strongly correlated the LASSO solution is not unique while the RIDGE solution is. As a result, neither LASSO nor RIDGE can deal with linearly dependent predictors while also achieving sparsity.

To solve this problem, we create a new estimator that combines the LASSO and RIDGE penalties. This is the idea behind Elastic Net. Given a mixture parameter $\alpha \in [0,1]$ and a regularization parameter $\lambda >0$, the Elastic Net estimator is given by;
$$\hat{\boldsymbol{\beta}}_{EN}=\text{argmin}_{\boldsymbol{\beta}} \left\{ \frac{1}{2n} \left\| \boldsymbol{Y} - \boldsymbol{X}\boldsymbol{\beta}  \right\|^2 _2 + \lambda [ \alpha \| \boldsymbol{\beta} \|_1 + (1- \alpha) \| \boldsymbol{\beta} \|_2 ] \right\}$$
Now we have an estimator that is capable of achieving sparsity and handling linearly dependent predictors at the same time. 

#Implementation
To analyze the performance of Elastic Net we will implement the estimator defined above in R.
Note: We will use the function \mathtt{cv.glmnet()} from the package \mathtt{glmnet} to select $\lambda$ by cross-validation and fit a Gaussian Elastic Net model with mixture parameter $\alpha$. The function takes as input a design matrix $X$, a response vector $Y$, the family of the error distribution (Gaussian for our purposes), and the mixture parameter $\alpha$. The function outputs a matrix containing the various properties of the models for $X$ and $Y$ fit with $\lambda$ as selected by CV and $\alpha$ as given.  
Below we write a function \mathtt{elastic.net.est(X, Y, alpha)} that fits an Elastic Net model for $X$ and $Y$, with $\alpha$ as given and $\lambda$ chosen by CV. First, we call \mathtt{cv.glmnet()} to select a sequence of $\lambda$ values, and fit the Elastic Net model for each. Then, we retrieve the minimum value of $\lambda$, and from the model fit with such $\lambda$, extract the estimated coefficients $\boldsymbol{\beta}$, number of predictors selected, fitted values, and mean squared error (MSE). The function then outputs the user-chosen $\alpha$, $\lambda$ as selected by CV, the number of predictors selected with $\lambda$, and the MSE of the model. 
```{r }
elastic.net.est<-function(X, Y, alpha){
  cross.val.fit<-cv.glmnet(X, Y, family="gaussian", alpha=alpha)
  lambda<-cross.val.fit$lambda.min
  coefs<-coef(cross.val.fit, s="lambda.min")
  fit<-glmnet(X, Y, "gaussian", alpha=alpha)
  plot(fit, xvar="lambda", label=TRUE)
  signif.pred<-length(which(coefs!=0))
  yhat<-cbind(1, X)%*%coefs
  SE<-mean((Y-yhat)^2)
  output<-cbind(alpha, lambda, signif.pred, SE)
  return(output)
}
```

Now, we can write a function to simulate data from a specified model where some of the predictors are correlated, and see how the Elastic Net estimate performs. The function \mathtt{simulate.elastic.net(n, p, d, rho, beta, alpha)} takes as input the number of observations (n), the number of covariates (p), the number of covariates that are correlated (d), the correlation between the d covariates (rho), the true values of $\boldsymbol{\beta}$ (beta), and a value of $\alpha$ (alpha). First, we build the $d \times d$ covariance matrix \mathtt{A} for the correlated variables and then we build the $p \times p$ covariance matrix \mathtt{Cov} with \mathtt{A} as the covariance of the first d variables. We then generate $X$ by simulating n observations from the p-dimensional multivariate normal distribution with mean 0 and covariance given by the matrix \mathtt{Cov}, and generate $Y$ from the specified model with standard normal errors. Finally, we call our function \mathtt{elastic.net.est(X, Y, alpha)} and return $\alpha$, $\lambda$, the number of predictors and MSE as before. 

```{r }
simulate.elastic.net<-function(n, p, d, rho, beta, alpha){
  A<-diag(1, d, d)
  for(i in 1:d){
    for(j in 1:d){
      if(j!=i){A[i,j]<-rho}}}
  Cov<-diag(1, p, p)
  for(i in 1:d){
    for( j in 1:d){
      Cov[i,j]<-A[i,j]}}
  X<-mvrnorm(n, rep(0, p), Cov)
  Y<-X%*%beta+rnorm(n)
  mod<-elastic.net.est(X, Y, alpha)
  return(mod)}
```

To better spot trends in the performance of Elastic net, it will also be useful to conduct multiple iterations of Elastic Net estimation simulating from the same multivariate normal distribution, and also compare performance as the correlation between covariates changes. We will write functions \mathtt{MonteCarlo.elastic.net(n, p, d, rho, beta, Alpha, K)} and \mathtt{vector.MonteCarlo.EN(n, p, d, Rho, beta, Alpha, K)} to do so now. First, we write

```{r }
MonteCarlo.elastic.net<-function(n, p, d, rho, beta, alpha, K){
  output<-cbind(1:K, 0, 0, 0, 0)
  for(k in 1:K){
    mod<-simulate.elastic.net(n, p, d, rho, beta, alpha)
    output[k,(2:5)]<-mod
  }
  return(output)
}

vector.MonteCarlo.EN<-function(n, p, d, Rho, beta, Alpha, K){
  results<-expand.grid(Alpha, Rho)
  results<-cbind(results, 0, 0, 0)
  colnames(results)<-c("Alpha", "Rho", "Lambda", "No. Variables", "MSE")
  for(i in 1:length(results[,1])){
    output<-cbind(1:K, 0, 0, 0, 0)
    for(k in 1:K){
      mod<-simulate.elastic.net(n, p, d, rho, beta, alpha)}
    
    
    mod<-MonteCarlo.elastic.net(n, p, d, results[i,2], beta, results[i,1], K)
    results[i, 3]<-mean(mod[,3])
    results[i, 4]<-mean(mod[,4])
    results[i, 5]<-mean(mod[,5])
  }
  return(results)
  
}




```

Now we can use this implementation to complete an example. Suppose we have a $n=100$ observations from the following model with $p=3$ where $X_1$ and $X_2$ have correlation $\rho=0.75$, and $X_3$ is uncorrelated. We find the Elastic Net estimates of $\boldsymbol{\beta}$ for three values of $\alpha$, $\alpha_1=0.1, \alpha_2=0.3, \alpha_3=0.9$, and compare.

$$Y=X\boldsymbol{\beta} + \boldsymbol{\epsilon},$$ 
$$\beta_1=2,\beta_2=2,\beta_3=3$$
```{r }
n<-100
p<-3
d<-2
beta<-c(2,2,3)
rho<-0.999
alpha1<-0.1
alpha2<-0.5
alpha3<-0.9

p3.rho75.LASSO<-simulate.elastic.net(n, p, d, rho, beta, 0)
p3.rho75.alpha1<-simulate.elastic.net(n, p, d, rho, beta, alpha1)
p3.rho75.alpha2<-simulate.elastic.net(n, p, d, rho, beta, alpha2)
p3.rho75.alpha3<-simulate.elastic.net(n, p, d, rho, beta, alpha3)
p3.rho75.RIDGE<-simulate.elastic.net(n, p, d, rho, beta, 1)



```
We can see as values of alpha decrease shrinkage decreases

Next, lets try a high-dimensional example. We take $n=100$ and $p=1000$ and generate the regression coefficients randomly from the interval [-1,1].
```{r }




```
```{r }
n<-100 #number of observations
beta<-c(1,2,1,3) #true values of beta
rho<-0.90 #values of cov(X_1, X_2)
alpha<-c(0.1, 0.3, 0.9) #values of alpha


elastic_net3<-function(n, beta, rho, alpha, K){
  num_alpha<-length(alpha)
  num_rho<-length(rho)
  Rho<-rep(0, num_alpha)                                                          #build table to hold squared errors
  Alpha<-rep(0,num_rho)
  NumPredictors<-rep(0, num_rho*num_alpha)
  MSE_K<-cbind(Rho, Alpha, NumPredictors, matrix(0, num_rho*num_alpha, K))
  
  for(k in 1:K){
    row=1
    for(j in 1:num_rho){
      cov<-matrix(data = c(1, rho[j], 0, rho[j], 1, 0, 0, 0, 1), nrow=3, ncol=3)  #build covariance matrix
      X<-mvrnorm(n, mu=c(0,0,0), Sigma =cov)                                      #simulate X
      design<-cbind(rep(1,n), X )
      Y<-design%*%beta+rnorm(n)                                                   #simulate Y
      for(i in 1:num_alpha){
        fit_i<-glmnet(X, Y, family="gaussian", alpha=alpha[i])                 #fit model with cv
        plot(fit_i, xvar="lambda", label=TRUE)
        fit_i_coef<-coef(fit_i, s="lambda.min")                                   #extract coefficients
        yhat<-design%*%fit_i_coef                                                 #extract fitted values
        if(k<=1){
          MSE_K[row,1]<-rho[j]
          MSE_K[row,2]<-alpha[i]                                                  #fill MSE table with alpha and rho
          MSE_K[row,3]<-length(fit_i_coef@x)}
        MSE_K[row,(k+3)]<-mean(((Y-yhat)^2))                                      #fill MSE table with squared errors
        row=row+1
      }}}
  results<-cbind(MSE_K[,(1:3)], 0)                                                #build results table
  if(K==1){
    results[,4]<-MSE_K[,4]
  }
  else{
  results[,4]<-rowMeans(MSE_K[,(4:(K+3))])                # fill results table with MSE
  }
  colnames(results)[4]<-"MSE"
  
  return(as.data.frame(results))
}

EN_p_3_one_iter<-elastic_net3(n, beta, rho, alpha,1)
ggplot(data=EN_p_3_one_iter, aes(x=Alpha, y=MSE))+geom_line(alpha=0.4)



```
Now, lets see what happens in the high-dimensional case. 

Suppose we have a dataset with $p=1000$ covariates, some of which are correlated, and $n=100$ observations. We perform Elastic net with $\alpha_1=0.1, \alpha_2=0.5, \alpha_3=0.9$ and $\lambda$ selected by cross-validation.  
```{r }
n<-100
p<-1000
beta<-c(runif(6,1,2), rep(0,95), runif(5, 1,2), rep(0,600), runif(5, 1,2), rep(0,290))
rho<-seq(0, 0.9, 0.1)
alpha<-seq(0, 1, 0.1)




elastic_net_p<-function(n, p, beta, rho, alpha, K){
  num_alpha<-length(alpha)
  num_rho<-length(rho)
  Rho<-rep(0, num_alpha)
  Alpha<-rep(0,num_rho)
  MSE<-rep(0, num_rho*num_alpha)
  NumPredictors<-rep(0, num_rho*num_alpha)
  
  MSE_K<-cbind(Rho, Alpha, NumPredictors, matrix(0, num_rho*num_alpha, K))
  
    for(k in 1:K){
    row=1
    for(j in 1:num_rho){
      cov<-matrix(rho[j], p, p)
      for(l in 1:p){
        cov[l,l]<-1}
      X<-mvrnorm(n, mu=c(0,0,0), Sigma =cov)
      design<-cbind(rep(1,n), X )
      Y<-design%*%beta+rnorm(n)
      for(i in 1:num_alpha){
        fit_i<-glmnet(X, Y, family="gaussian", alpha=alpha[i])
        fit_i_coef<-coef(fit_i, s="lambda.min")
        yhat<-design%*%fit_i_coef
        if(k<=1){
          MSE_K[row,1]<-rho[j]
          MSE_K[row,2]<-alpha[i]
          MSE_K[row,3]<-length(fit_i_coef@x)}
        MSE_K[row,(k+3)]<-mean(((Y-yhat)^2))
        row=row+1
      }}}
  results<-cbind(MSE_K[,(1:3)], 0)
  results[,4]<-rowMeans(MSE_K[,(4:K)])
  colnames(results)[4]<-"MSE"
  return(as.data.frame(results))
}

lambda<-matrix(0,length(alpha),length(rho))
mse<-matrix(0,length(alpha),length(rho))
q<-matrix(0,length(alpha), length(rho))



for(j in 1:length(rho)){
  cov=matrix(c(1, rho[j], rho[j], rho[j], rho[j], 
              rho[j], 1, rho[j], rho[j], rho[j],
              rho[j], rho[j], 1, rho[j], rho[j],
              rho[j], rho[j], rho[j], 1, rho[j],
              rho[j], rho[j], rho[j], rho[j], 1), nrow=5, ncol=5)
  X1_5<-mvrnorm(n, mu=c(0,0,0,0,0), Sigma =cov )
  X6_1000<-mvrnorm(n, mu=rep(0, 995), Sigma = diag(rep(1, 995)))
  X<-cbind(X1_5, X6_1000)
  design<-cbind(rep(1,n), X)
  Y<-design%*%beta+rnorm(n)

  for(i in 1:length(alpha)){
    fit_i<-cv.glmnet(X, Y, family="gaussian", alpha=alpha[i])
    lambda[i,j]<-fit_i$lambda.min
    fit_i_coef<-coef(fit_i, s=lambda[i])
    yhat<-design%*%fit_i_coef
    mse[i,j]<-mean((Y-yhat)^2)
  }}



```

```{r }
n<-100
beta<-c(5,1,3,2)
rho<- seq(-0.9, 0.9, 0.25)
alpha<-seq(0.1, 0.9, 0.1)

elastic_net3<-function(n, beta, rho, alpha, K){
  num_alpha<-length(alpha)
  num_rho<-length(rho)
  Rho<-rep(0, num_alpha)
  Alpha<-rep(0,num_rho)
  MSE<-rep(0, num_rho*num_alpha)
  NumPredictors<-rep(0, num_rho*num_alpha)
  
  MSE_K<-cbind(Rho, Alpha, NumPredictors, matrix(0, num_rho*num_alpha, K))
  
  for(k in 1:K){
    row=1
    for(j in 1:num_rho){
      cov<-matrix(data = c(1, rho[j], 0, rho[j], 1, 0, 0, 0, 1), nrow=3, ncol=3)
      X<-mvrnorm(n, mu=c(0,0,0), Sigma =cov)
      design<-cbind(rep(1,n), X )
      Y<-design%*%beta+rnorm(n)
      for(i in 1:num_alpha){
        fit_i<-cv.glmnet(X, Y, family="gaussian", alpha=alpha[i])
        fit_i_coef<-coef(fit_i, s="lambda.min")
        yhat<-design%*%fit_i_coef
        if(k<=1){
          MSE_K[row,1]<-rho[j]
          MSE_K[row,2]<-alpha[i]
          MSE_K[row,3]<-length(fit_i_coef@x)}
        MSE_K[row,(k+3)]<-mean(((Y-yhat)^2))
        row=row+1
      }}}
  results<-cbind(MSE_K[,(1:3)], 0)
  results[,4]<-rowMeans(MSE_K[,(4:K+3)])
  colnames(results)[4]<-"MSE"
  
  return(as.data.frame(results))
}
#hundered_iter<-elastic_net3(n, beta, rho, alpha, 100)
#fifty_iter<-elastic_net3(n, beta, rho, alpha, 50)
ten_iter<-elastic_net3(n, beta, rho, alpha, 10)

ggplot(data=hundered_iter, aes(x=Rho, y=MSE, group=factor(Alpha)))+geom_line(aes(colour=factor(Alpha)),  alpha=0.4)
ggplot(data=fifty_iter, aes(x=Rho, y=MSE, group=factor(Alpha)))+geom_line(aes(colour=factor(Alpha)),  alpha=0.4)
ggplot(data=ten_iter, aes(x=Rho, y=MSE, group=factor(Alpha)))+geom_line(aes(colour=factor(Alpha)),  alpha=0.4)




```